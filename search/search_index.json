{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apache Kafka Streams Sampless This repository regroup a set of personal studies and quick summary on Kafka. Run Kafka 2.5 locally for development The docker compose starts one zookeeper and two kafka brokers locally on the kafkanet network. To start kafkacat using the debezium tooling docker run --tty --rm -i --network kafkanet debezium/tooling:latest If you run with Event Streams on Cloud set the KAFKA_BROKERS and KAFKA_APIKEY environment variables accordingly. Kafka Streams Samples kstreams-getting started In the kstreams-getting-started we use the TopologyTestDriver to implement a set of simple labs from simple topology to more complex solution. Basically going under the src/test/java folder and go lab by lab should be sufficient. This project was created with mvn archetype:generate -DgroupId=ibm.gse.eda -DartifactId=kstreams-getting-started -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.4 -DinteractiveMode=false To test the different labs run mvn test . Some summary of what the labs do and some of the concepts and reference to material sources is in this note . Kstreams stateful The Kstreams-stateful folder includes tests for working with ktables, store, aggregates... with unit tests and integration tests. Joining streams with time windows The streams-joining-sample includes a simple java based Open Liberty app to listen to 4 streams of data to join within a time windows. Kafka with Quarkus reactive messaging The Quarkus reactive msg folder includes a clone of the code from , slighlty adapted. ./mvnw compile quarkus:dev to start local quarkus app and continuously develop. Here is a template code for quarkus based Kafka consumer: quarkus-event-driven-consumer-microservice-template . Read this interesting guide with Quarkus and kafka streaming: Quarkus using Kafka Streams , which is implemented in the quarkus-reactive-msg producer, aggregator folders. To generate the starting code for the producer we use the quarkus maven plugin with kafka extension: mvn io.quarkus:quarkus-maven-plugin:1.4.1.Final:create -DprojectGroupId=jbcodeforce.kafka.study -DprojectArtifactId=producer -Dextensions=\"kafka\" for the aggregator: mvn io.quarkus:quarkus-maven-plugin:1.4.1.Final:create -DprojectGroupId=jbcodeforce.kafka.study -DprojectArtifactId=aggregator -Dextensions=\"kafka-streams,resteasy-jsonb\" Interesting how to generate reference value to a topic with microprofile reactive messaging. stations is a hash: @Outgoing ( \"weather-stations\" ) public Flowable < KafkaRecord < Integer , String >> weatherStations () { List < KafkaRecord < Integer , String >> stationsAsJson = stations . stream () . map ( s -> KafkaRecord . of ( s . id , \"{ \\\"id\\\" : \" + s . id + \", \\\"name\\\" : \\\"\" + s . name + \"\\\" }\" )) . collect ( Collectors . toList ()); return Flowable . fromIterable ( stationsAsJson ); }; Channels are mapped to Kafka topics using the Quarkus configuration file application.properties . To build and run: # under producer folder docker build -f src/main/docker/Dockerfile.jvm -t quarkstream/producer-jvm . # under aggregator folder docker build -f src/main/docker/Dockerfile.jvm -t quarkstream/aggregator-jvm . # Run under quarkus-reactive-msg docker-compose up # Run kafkacat docker run --tty --rm -i --network kafkanet debezium/tooling:1.0 $ kafkacat -b kafka1:9092 -C -o beginning -q -t temperatures-aggregated","title":"Introduction"},{"location":"#apache-kafka-streams-sampless","text":"This repository regroup a set of personal studies and quick summary on Kafka.","title":"Apache Kafka Streams Sampless"},{"location":"#run-kafka-25-locally-for-development","text":"The docker compose starts one zookeeper and two kafka brokers locally on the kafkanet network. To start kafkacat using the debezium tooling docker run --tty --rm -i --network kafkanet debezium/tooling:latest If you run with Event Streams on Cloud set the KAFKA_BROKERS and KAFKA_APIKEY environment variables accordingly.","title":"Run Kafka 2.5 locally for development"},{"location":"#kafka-streams-samples","text":"","title":"Kafka Streams Samples"},{"location":"#kstreams-getting-started","text":"In the kstreams-getting-started we use the TopologyTestDriver to implement a set of simple labs from simple topology to more complex solution. Basically going under the src/test/java folder and go lab by lab should be sufficient. This project was created with mvn archetype:generate -DgroupId=ibm.gse.eda -DartifactId=kstreams-getting-started -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.4 -DinteractiveMode=false To test the different labs run mvn test . Some summary of what the labs do and some of the concepts and reference to material sources is in this note .","title":"kstreams-getting started"},{"location":"#kstreams-stateful","text":"The Kstreams-stateful folder includes tests for working with ktables, store, aggregates... with unit tests and integration tests.","title":"Kstreams stateful"},{"location":"#joining-streams-with-time-windows","text":"The streams-joining-sample includes a simple java based Open Liberty app to listen to 4 streams of data to join within a time windows.","title":"Joining streams with time windows"},{"location":"#kafka-with-quarkus-reactive-messaging","text":"The Quarkus reactive msg folder includes a clone of the code from , slighlty adapted. ./mvnw compile quarkus:dev to start local quarkus app and continuously develop. Here is a template code for quarkus based Kafka consumer: quarkus-event-driven-consumer-microservice-template . Read this interesting guide with Quarkus and kafka streaming: Quarkus using Kafka Streams , which is implemented in the quarkus-reactive-msg producer, aggregator folders. To generate the starting code for the producer we use the quarkus maven plugin with kafka extension: mvn io.quarkus:quarkus-maven-plugin:1.4.1.Final:create -DprojectGroupId=jbcodeforce.kafka.study -DprojectArtifactId=producer -Dextensions=\"kafka\" for the aggregator: mvn io.quarkus:quarkus-maven-plugin:1.4.1.Final:create -DprojectGroupId=jbcodeforce.kafka.study -DprojectArtifactId=aggregator -Dextensions=\"kafka-streams,resteasy-jsonb\" Interesting how to generate reference value to a topic with microprofile reactive messaging. stations is a hash: @Outgoing ( \"weather-stations\" ) public Flowable < KafkaRecord < Integer , String >> weatherStations () { List < KafkaRecord < Integer , String >> stationsAsJson = stations . stream () . map ( s -> KafkaRecord . of ( s . id , \"{ \\\"id\\\" : \" + s . id + \", \\\"name\\\" : \\\"\" + s . name + \"\\\" }\" )) . collect ( Collectors . toList ()); return Flowable . fromIterable ( stationsAsJson ); }; Channels are mapped to Kafka topics using the Quarkus configuration file application.properties . To build and run: # under producer folder docker build -f src/main/docker/Dockerfile.jvm -t quarkstream/producer-jvm . # under aggregator folder docker build -f src/main/docker/Dockerfile.jvm -t quarkstream/aggregator-jvm . # Run under quarkus-reactive-msg docker-compose up # Run kafkacat docker run --tty --rm -i --network kafkanet debezium/tooling:1.0 $ kafkacat -b kafka1:9092 -C -o beginning -q -t temperatures-aggregated","title":"Kafka with Quarkus reactive messaging"},{"location":"kstreams/","text":"Kafka Streams Getting started In this article we are presenting how to use the Kafka Streams API combined with Kafka event sourcing to implement different simple use cases. The use cases are implemented inside the test folder. Some of the domain classes are defined in the src/java folder. Streams topology are done in the unit test but could also be part of a service class to be used as an example running with kafka. Lab 1 Getting started to use TopologyTestDriver The goal of this first example is to understand how to use the Kafka TopologyTestDriver API to test any Kafka Streams topology. This sample is a simplest version of the Confluent example defined here . Streams topology could be tested outside of Kafka run time environment using the TopologyTestDriver. Each tests define the following: a simple configuration for the test driver, input and output topics a topology to test a set of tests to define data to send to input topic and assertions from the output topic. mask credit card number Using a simple kstream to change data on the fly, like for example encrypt a credit card number. Test is EncryptCreditCardTest . This is the first test to use the TopologyTestDriver class to run business logic outside of kafka. The class uses org.apache.kafka.common.serialization.Serdes and String serdesm and a JSON serdes for the domain class Purchase. The other interesting approach is to use domain object with builder class and DSL to define the model: Purchase . builder ( p ). maskCreditCard (). build () Joining 3 streams with reference data to build a document This is a simple example of joining 3 sources of kafka streams to build a merged document, with some reference data loaded from a topic: The shipment status is a reference table and loaded inside a kafka topic: shipment-status The order includes items and customer id reference Customer is about the customer profile Products is about products inventory. The outcome is an order report document that merge most of the attributes of the 3 streams. How streams flows are resilient? Specifying the replicas factor at the topic level, with a cluster of kafka brokers, combine with transactional event produce, ensure to do not lose messages. The producer client code has the list of all the brokers to contact in case of failure and will try to connect to any broker in the list. How to scale? Further readings Kafka Streams\u2019 Take on Watermarks and Triggers what continuous refinement with operational parameters means:","title":"Kafka streaming notes"},{"location":"kstreams/#kafka-streams-getting-started","text":"In this article we are presenting how to use the Kafka Streams API combined with Kafka event sourcing to implement different simple use cases. The use cases are implemented inside the test folder. Some of the domain classes are defined in the src/java folder. Streams topology are done in the unit test but could also be part of a service class to be used as an example running with kafka.","title":"Kafka Streams Getting started"},{"location":"kstreams/#lab-1-getting-started-to-use-topologytestdriver","text":"The goal of this first example is to understand how to use the Kafka TopologyTestDriver API to test any Kafka Streams topology. This sample is a simplest version of the Confluent example defined here . Streams topology could be tested outside of Kafka run time environment using the TopologyTestDriver. Each tests define the following: a simple configuration for the test driver, input and output topics a topology to test a set of tests to define data to send to input topic and assertions from the output topic.","title":"Lab 1 Getting started to use TopologyTestDriver"},{"location":"kstreams/#mask-credit-card-number","text":"Using a simple kstream to change data on the fly, like for example encrypt a credit card number. Test is EncryptCreditCardTest . This is the first test to use the TopologyTestDriver class to run business logic outside of kafka. The class uses org.apache.kafka.common.serialization.Serdes and String serdesm and a JSON serdes for the domain class Purchase. The other interesting approach is to use domain object with builder class and DSL to define the model: Purchase . builder ( p ). maskCreditCard (). build ()","title":"mask credit card number"},{"location":"kstreams/#joining-3-streams-with-reference-data-to-build-a-document","text":"This is a simple example of joining 3 sources of kafka streams to build a merged document, with some reference data loaded from a topic: The shipment status is a reference table and loaded inside a kafka topic: shipment-status The order includes items and customer id reference Customer is about the customer profile Products is about products inventory. The outcome is an order report document that merge most of the attributes of the 3 streams.","title":"Joining 3 streams with reference data to build a document"},{"location":"kstreams/#how-streams-flows-are-resilient","text":"Specifying the replicas factor at the topic level, with a cluster of kafka brokers, combine with transactional event produce, ensure to do not lose messages. The producer client code has the list of all the brokers to contact in case of failure and will try to connect to any broker in the list.","title":"How streams flows are resilient?"},{"location":"kstreams/#how-to-scale","text":"","title":"How to scale?"},{"location":"kstreams/#further-readings","text":"Kafka Streams\u2019 Take on Watermarks and Triggers what continuous refinement with operational parameters means:","title":"Further readings"}]}